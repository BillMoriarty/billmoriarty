{"keys":[{"path":["title"],"id":"title","weight":1,"src":"title"},{"path":["body"],"id":"body","weight":1,"src":"body"}],"records":[{"i":0,"$":{"0":{"v":"This page has not yet sprouted","n":0.408},"1":{"v":"[Dendron](https://dendron.so/) (the tool used to generate this site) lets authors selective publish content. You will see this page whenever you click on a link to an unpublished page\n\n![](https://foundation-prod-assetspublic53c57cce-8cpvgjldwysl.s3-us-west-2.amazonaws.com/assets/images/not-sprouted.png)","n":0.189}}},{"i":1,"$":{"0":{"v":"Home","n":1},"1":{"v":"Hello. I work as software developer & research associate focusing on augmented reality and virtual reality in the [GT Applied Research team](https://www.jpmorgan.com/technology/applied-research).\n\nTrying out  <a rel=\"me\" href=\"https://mastodon.social/@billmoriarty\">Mastodon</a>\n\nI studied computer science and software design at Temple University.\n\nI have made a lot of recordings as a record producer, recording engineer, mixing engineer, and sometimes a musician. You may have even heard some of them.\n\nOld podcast interviewing people changing careers into tech:\nhttps://anchor.fm/possible-paths\n\nMy partner is an artist & UX designer.\n\nWe are the parents of one kid.\n\nI spend a lot of time volunteering with The Wisdom Seat: a group of Buddhist teachers and practitioners who host spiritual teachers, sponsor meditation retreats and offer support to people on the Buddhist path.\n\nThe Wisdom Seat\nhttps://thewisdomseat.org\n\nPodcast for The Wisdom Seat\nhttps://anchor.fm/the-wisdom-seat\n\nRecording was my first love. At age 13 I got a guitar and a 4 track recorder. I have been making recordings as long as I‚Äôve been around music.\n\nHear my own music here ///Ô∏è\nhttps://open.spotify.com/artist/1eIs7Nuo5VeloBHoYr9fAB?si=H2TkGmQxSniZV_Wsii3Bag\nhttps://billmoriarty.bandcamp.com\n\nI will keep playing with recording until I‚Äôm dead üíÄüíÄüíÄ\n\nI have written some articles for one of my favorite magazines: [TapeOpÔ∏è]([https://tapeop.com/searches/194699/](https://tapeop.com/searches/194699/)) \nhttps://tapeop.com/searches/194699/\n\n\n\n<iframe src=\"https://relentless-architect-4898.ck.page/5ddeb58ffe\" style=\"width:100%; height:500px; border:none; \"></iframe>\n\n![Studio](/assets/images/thespot.jpg)\n","n":0.075}}},{"i":2,"$":{"0":{"v":"Recordings","n":1},"1":{"v":"\nI made a lot of recordings as a recording engineer, mixing engineer, and some as a record producer. I'll post them here as I find which still are playable online.\n","n":0.183}}},{"i":3,"$":{"0":{"v":"Casco Bay, by Bill Moriarty","n":0.447},"1":{"v":"<iframe style=\"border: 0; width: 400px; height: 439px;\" src=\"https://bandcamp.com/EmbeddedPlayer/album=777549037/size=large/bgcol=333333/linkcol=ffffff/artwork=small/transparent=true/\" seamless><a href=\"https://billmoriarty.bandcamp.com/album/casco-bay\n\">Casco Bay | Bill Moriarty</a></iframe>\n","n":0.267}}},{"i":4,"$":{"0":{"v":"Born And Thrown On A Hook, by Drink Up Buttercup","n":0.316},"1":{"v":"<iframe style=\"border: 0; width: 400px; height: 439px;\" src=\"https://bandcamp.com/EmbeddedPlayer/album=939058769/size=large/bgcol=333333/linkcol=ffffff/artwork=small/transparent=true/\" seamless><a href=\"https://drinkupbuttercupphilly.bandcamp.com/album/born-and-thrown-on-a-hook\n\">Born And Thrown On A Hook | Drink Up Buttercup</a></iframe>\n","n":0.229}}},{"i":5,"$":{"0":{"v":"Beyond the Station, by Annabelle''s Curse","n":0.408},"1":{"v":"<iframe style=\"border: 0; width: 400px; height: 439px;\" src=\"https://bandcamp.com/EmbeddedPlayer/album=3057430171/size=large/bgcol=333333/linkcol=ffffff/artwork=small/transparent=true/\" seamless><a href=\"https://annabellescurse.bandcamp.com/album/beyond-the-station\n\">Beyond the Station | Annabelle's Curse</a></iframe>\n","n":0.258}}},{"i":6,"$":{"0":{"v":"Awoken, by Bigtree Bonsai","n":0.5},"1":{"v":"<iframe style=\"border: 0; width: 400px; height: 439px;\" src=\"https://bandcamp.com/EmbeddedPlayer/album=2585170884/size=large/bgcol=333333/linkcol=ffffff/artwork=small/transparent=true/\" seamless><a href=\"https://bigtreebonsai.bandcamp.com/album/awoken\n\">Awoken | Bigtree Bonsai</a></iframe>\n","n":0.277}}},{"i":7,"$":{"0":{"v":"Asleep In My Shoes, by Joe D'Amico","n":0.378},"1":{"v":"\nThe song \"Asleep in my shoes\" from this album is one of my favorite's I've had the pleasure to mix. I felt 100% in sync with the lyrics and sentiment here. It felt like he was singing what was in my mind. And I love how Joe brings the countdown theme from the chorus in the second verse. \n\nThe climbing harmonies he picked for \"One day\" match the aspiration in the lyrics. \n\nI'm happy with the mix, which is rare for me.\n\n<iframe style=\"border: 0; width: 400px; height: 439px;\" src=\"https://bandcamp.com/EmbeddedPlayer/album=778436876/size=large/bgcol=333333/linkcol=ffffff/artwork=small/transparent=true/\" seamless><a href=\"https://joedamico.bandcamp.com/album/asleep-in-my-shoes\n\">Asleep In My Shoes | Joe D'Amico</a></iframe>\n","n":0.102}}},{"i":8,"$":{"0":{"v":"Ali Wadsworth, by Ali Wadsworth","n":0.447},"1":{"v":"<iframe style=\"border: 0; width: 400px; height: 439px;\" src=\"https://bandcamp.com/EmbeddedPlayer/album=1497395358/size=large/bgcol=333333/linkcol=ffffff/artwork=small/transparent=true/\" seamless><a href=\"https://aliwadsworth.bandcamp.com/album/ali-wadsworth\n\">Ali Wadsworth | Ali Wadsworth</a></iframe>\n","n":0.267}}},{"i":9,"$":{"0":{"v":"XR Resources","n":0.707}}},{"i":10,"$":{"0":{"v":"Tim Dwyer","n":0.707},"1":{"v":"\nThese are notes from listening to Immersive Analytics with Tim Dwyer, a podcast interview on Data Stories 2019. These are free-form and I make no guarantee of  correct grammar. \n\nHe started studying this field around 2001\n\n> My take: We‚Äôre still, in 2022, kinda in the same place as described here...\nimagining AR as a way to look at a physical object and seeing or hearing information\n\nAn AR headset can talk to the output of an IoT connected machine\n- Get that info into your headset\n\n\nAim to use natural gestures\n\nThey simulate Augmented Reality environments in virtual reality. \nThe VR devices have a wider field of view.\nThe controllers you get with VR devices track very accurately.\n\nThey create screens hanging in the virtual space - they're interested in the combination of the screens.\n\nThat allows us to imagine the future more than working in the current limitations of the AR technology.\n\n- The Human Computer Interaction\n- Filtering in data visualization \n\nWhat does a ‚Äòslider‚Äô look like in VR???\nThe way this was solved on the desktop needs to be rethought \n\nToday is like the 1980's\n- people explored a lot of kinds of computer mouse until they found what actualyl works\n- many people don't know how much university work went into that kind of research \n\n\nThere are a *lot* of challenges to solve \n\n### \"Flow-maps\"\nYalang Yang (check spelling )\n‚ÄúMaps of movements of people, or trader, overlaid on a cartographic map.‚Äù\nThis has been discussed in the visualization community for a long time. \nIn the 19th century there\nMore recently they were generated automatically onto 2d maps\nWe show flows on the globe. You have the classic problem \n\n> side note from Bill...wow‚Ä¶I'm just remembering I‚Äôve had this dream since I was a young kid - seeing a slowly spinning globe in front of you that you can zoom in and out of, and see very detailed, animated information as you move in.\n\nread Yolang‚Äôs work \n\n\nIn 3D, you can map 3 dimensions to x,y,z\n\n\n- Perspective distortion\n- Occlusion\n\nUsing haptics\n- you can \"feel\" holes in the data\n- or feel the data as a wall\n- the takeaway from that paper (note: find the paper)both of those haptic interactions improve your ability to see inside dense clouds of data\n\n\n- Windows mixed reality framework \n\n\nImmersive Analytics Toolkit\nMaxime (Maxine?)\nIATK \n- github\n\nInfoVis from 2018 also presented other toolkits.\n\n> Host: 'we have go through a time when people try crazy things!'\n\n### collaborative data visualization \nVR: data instantiated in the world around and walk around it\n\n\n\nOther people to look up\n- Christoph Herder\n- Natalie (Enrie Riche?)  (at Microsoft research)\n- Steve Drucker\n- Benjamin Buck\n\n","n":0.049}}},{"i":11,"$":{"0":{"v":"Unity","n":1},"1":{"v":"\nAs of Feb 3, 2022 at 4:37:04 PM I have started to learn Unity for VR and AR. They call it XR. All the R's.\nI'll post everything I find that is useful here. \n","n":0.171}}},{"i":12,"$":{"0":{"v":"Physics","n":1}}},{"i":13,"$":{"0":{"v":"Rigidbody","n":1},"1":{"v":"https://www.youtube.com/watch?v=WTGcs10Sj34\n\nThis allows a [[Unity.Game Object]] to\n- fall with gravity \n- have mass\n- have drag\n- have velocity\n\nThe [[Unity.Game Object]] must also have a [[Unity.Collider]] attached.\n\n### Mass\nAffects how collisions are treated. They react less with a higher mass.\n\n### Drag\nAffects how quickly the object will slow down without other interactions. \n>think of it like air resistance.\nThe rate of loss of linear velocity.\n\n### Angular Drag\nHow fast it will slow its rotation.\n\n### Use Gravity\n- click this to enable gravity\n- this is a 3D vector\n- Edit -> Projects Settings -> Gravity\n\n### Is Kinematic\n- whether or not a rigid body will react to physics\n- if it Is Kinematic\n- - this uses less compute\n- - does not require the Physics Engine to reevaluate the entire scene\n- you can move these objects with Transforms\n- constraints\n- - Freeze Position\n- - Freeze Rotation\n\n\nAt the start of a scene, the physics engine checks the state of all static objects.\n\nIf an object Is Kinematic\n- it informs the game engine of its location\nIf na object is not Kinematic\n- the game engine evaluates and dictates where the object should be \n\nUse the Interpolate setting to smooth jitter based on the prior frame\nUse the Extrapolate setting to smooth jitter based on the predicted frame\n","n":0.071}}},{"i":14,"$":{"0":{"v":"asset","n":1},"1":{"v":"\nAssets are the files that make up your project, such as Prefab GameObjects, textures, and audio.","n":0.25}}},{"i":15,"$":{"0":{"v":"XR Grab Interactable","n":0.577},"1":{"v":"\nWhen you instantiate this on a GameObject in order to grab it with VR hands, it will install a [[Unity.Physics.Rigidbody]]. The rigidbody cannot be removed, with the prompt showing:\n> \"Can't remove Rigidbody because XRGrabInteractable (Script) depends on it\"\n","n":0.162}}},{"i":16,"$":{"0":{"v":"Vuforia","n":1},"1":{"v":"\nThis is an augmented reality library for Unity.\n\nVuforia goes straight to the hardware's camera. It only uses the camera.\n\nPTC bought Vuforia from Qualcomm.\n\nPlayer Settings\n![](/assets/images/2022-03-09-11-09-04.png)\n\n## Errors & How to Solve Them\n\n`[ufw runUIApplicationMainWithArgc: argc argv: argv]; `\n\n> For anyone else that runs into this error the solution was two-fold. \n- You have to uncheck \"Strip Engine Code\" in Unity under player settings > IOS. \n- Then in Xcode, you have to find (under either Swift or Apple Clang - code generation) a setting called 'Optimization Level' and set it to none.\nhttps://forum.unity.com/threads/xcode-error-when-building-thread-1-exc_bad_access.802374/","n":0.106}}},{"i":17,"$":{"0":{"v":"Virtual Reality Setup","n":0.577},"1":{"v":"\nImport the XR Interaction toolkit\n![](/assets/images/xr_setup_1.png)\n\n---\n\nFrom 'Starter Assets'\n![](/assets/images/xr_setup_1a.png)\n\n---\n\nIn preset manager, name these two fields something obvious like Left and Right.\n![](/assets/images/xr_setup_preset_manager.png)\n\n---\n\nTo make this Virtual Reality capable, \n- delete the main camera if it exists\n- add an XR Origin.\n\n![](/assets/images/add_xr_origin.png)\n\n---\n\n### In the XR Origin\n- Add Component\n- - Input Action Manager\n\n![](/assets/images/xr_setup_2.png)\n\nUnder Action Assets\n- add the Default Input Action for the hardware device \n- [ ] I wonder if this is why my simulator controls are not working as expected?    \n![](/assets/images/default_input_action.png)\n\n---\n\n![](/assets/images/xr_setup_3.png)\n\n![](/assets/images/xr_setup_4.png)\n\n![](/assets/images/xr_setup_5.png)\n\n![](/assets/images/xr_setup_6.png)\n\n![](/assets/images/xr_setup_7.png)\n\n\nhttps://www.youtube.com/watch?v=Fk9h1mmd7QQ&list=PLiA2c8SsRca80fE66y0tto1jpq4f_ahE6&index=1\n\n\nhttps://www.youtube.com/watch?v=9aitPyydELY&list=PLiA2c8SsRca80fE66y0tto1jpq4f_ahE6&index=2\n\nhttps://www.youtube.com/watch?v=3M8e-S8eVe0&list=PLiA2c8SsRca80fE66y0tto1jpq4f_ahE6&index=3\n","n":0.115}}},{"i":18,"$":{"0":{"v":"Video Playback","n":0.707},"1":{"v":"\nTo play a video on a game object, \n\nVideo Player is influencing the Texture \n\n\n\nMake sure the video player is\n![](/assets/images/video_1.png)\n\nMake a new Material which is Unlit \nUnlit means don't take into account lighting in the environment.\n![](/assets/images/video_2.png)\n\nChange the default behaviors of the video playback so that when the image tracker no longer sees the image, it stops playback.\n![](/assets/images/video_3.png)\n\n![](/assets/images/video_4.png)\n\n![](/assets/images/video_5.png)\n\n\nTo make the game object be transparent before the video plays.\nUse this transparent png as a Texture for the game object which will play the video\n![]/assets/images/video_6.png\n![](/assets/images/video_6.png)\n\n\n\n---\n\n","n":0.11}}},{"i":19,"$":{"0":{"v":"VR Text","n":0.707},"1":{"v":"\nSetting up text in VR\n\n\n![](/assets/images/vr_text_1.png)\n\n![](/assets/images/vr_text_2.png)\n\n![](/assets/images/vr_text_3.png)\n\n![](/assets/images/vr_text_4.png)\n\n![](/assets/images/vr_text_5.png)\n","n":0.447}}},{"i":20,"$":{"0":{"v":"VR Hands","n":0.707},"1":{"v":"\n## Notes from [Usman Mir](https://www.usmanmir.com)\n\n### XR Interaction ToolKit Setup\n\n1. Package Manager Install XR Interaction ToolKit\n2. XR Interaction Tool Kit Package > Samples > Import Starter Assets (Optional Simulator for non VR Headset Users)\n3. Project Window> Samples>XR Interaction Toolkit > 2.0.0 > Starter Assets then select the asset ‚ÄúXRI Default Left Controller‚Äù (as well as right) and in the Inspector click ‚ÄúAdd to ActionBasedController default‚Äù at the top of the inspector window\n4. Edit>Project Settings>Preset Manager then type in ‚ÄúRight‚Äù and ‚ÄúLeft‚Äù to the left of the appropriate Preset input field\n5. In the Hierarchy window add XR> XR Origin (Action Based)\n6. Add an ‚ÄúInput Action Manager‚Äù Component to the XR Rig\n7. Add to the action assets list the ‚ÄúXRI Default Input Actions‚Äù asset\n8. Edit>Project Settings>XR Plug-in Management and enable desired platforms\n\n\n### Misc.\n- Add Grab Interactables and rigidbodies and colliders to objects you want to pick up\n- You can replace Line and Ray Interactors from Hands with Direct Interactors to reach out and grab objects instead of pointing and clicking\n- Add Visuals for Hands by adding something into the model prefab property of each Controller component on the hands\n\n\n![](/assets/images/locomotion_1.jpg)\n\n![](/assets/images/locomotion_2.jpg)\n\n![](/assets/images/locomotion_3.jpg)\n\n![](/assets/images/locomotion_4.jpg)","n":0.073}}},{"i":21,"$":{"0":{"v":"Unsorted Unity Notes","n":0.577},"1":{"v":"\n*These notes need to be edited & sorted*\n\nWhat is a \"complicated particle system?\"\n\n\nUnity Vs Unreal\n- unity: larger community for helping learn\n- unity: more documentation\n- unreal: looks beautiful by default\n- - has enabled by default\n- Unity: is trying to stand out by making themselves the XR development suite\n\n\nshader graph for altering the mesh\n- [ ] what is meant here by shader?\n- [ ] what is meant here by graph?\n- [ ] what is meant here by mesh?\n\n\nUnity Hub\n- icons under each Unity install indicates what build modules are installed for that \n\nIf you don't have \"Add Modules\" available, that is because you \"located\" that version of Unity instead of installing it\n\n\nThey *highly* recommend you go through the\n\"roll-a-ball\" tutorial:\n- [ ] https://learn.unity.com/project/roll-a-ball\n\n\nMake full use of the Unity Community\n- https://unity.com/community\n\nNew project\n- first choice are Render Options\n3D: this is the standard render pipeline\nHDRP\n- use this for Desktop experiences\nUniversal REnder pipeline\n- use this in gerneal \n\n\nCamera\n- audio listener\n\nSelect a game object\n- type 'f'\n- it zooms in to that object \n\n\nShift + Option + Command + click & drag\n\nOption + click & drag\n\nNew Game Object\n- Unity will place the object at the focus area\n- people mostly don't use transforms, they manipulate in the scene view\n\n\n[[Unity.Mesh Renderer]]\n\n\n[[Unity.Material]]\n\n\n[[Unity.Texture]]\n\n\nGlobal vs Local\nglobal: what the world considers to be forward\nlocal:\n\n\n\nhold down Command to snap in 15 degree elements\n\n---\n\nParent -> Child relationships\n\nIf the Game Object is a child of the Parent\nAnd you edit the scale, rotation and position of the Parent, then the Child is moved proportionally.\n\nBut when you modify the Parent's rotation such as -90, the Child's rotation can still say 0, because that is its local rotation.\n\nPivot point is at the feet of a character.\nCenter position is usually near their torso.\n\nHow to snap Game Object to he plane\n- hold down V\n\n\nIf you duplicate a [[Unity.Game Object]], and you want to edit it, you first have to assign it a material. \nOtherwise, if you edit it, it effects the original gameobject which was duplicated.\n\n\n\n### Prefab Variants\nA key concept\nhttps://docs.unity3d.com/Manual/PrefabVariants.html\n\n\nIf you have too many start or awake functions, your app takes a long time to start.\n\n---\n\nREcommendations for developing VR for Uculus\nhttps://developer.oculus.com/resources/publish-quest-req/\n\n\n\n--\n\nProcedural generation\n\nhttps://www.youtube.com/watch?v=64NblGkAabk&ab_channel=Brackeys\n\nhttps://www.youtube.com/watch?v=y6KwsRkQ86U&ab_channel=DevOrenge\n\n--\n\ncollider\nrigidBody\n\nXR grab interactable\n\nonTriggerEnter\n\n---\n\n\nXR Interaction ToolKit Setup\n\nPackage Manager Install XR Interaction ToolKit\nXR Interaction Tool Kit Package > Samples > Import Starter Assets (Optional Simulator for non VR Headset Users)\nProject Window> Samples>XR Interaction Toolkit > 2.0.0 > Starter Assets then select the asset ‚ÄúXRI Default Left Controller‚Äù (as well as right) and in the Inspector click ‚ÄúAdd to ActionBasedController default‚Äù at the top of the inspector window\nEdit>Project Settings>Preset Manager then type in ‚ÄúRight‚Äù and ‚ÄúLeft‚Äù to the left of the appropriate Preset input field\nEdit>Project Settings>XR Plug-in Management and enable desired platforms\nIn the Hierarchy window add XR> XR Origin (Action Based)\nAdd an ‚ÄúInput Action Manager‚Äù Component to the XR Rig\nAdd to the action assets list the ‚ÄúXRI Default Input Actions‚Äù asset\n\nAdd Grab Interactables and rigidbodies and colliders to objects you want to pick up\nAdd Visuals for Hands by adding something into the model prefab property of each Controller component on the hands Or make them children of the controller and animate with events\nYou can replace Line and Ray Interactors from Hands with Direct Interactors to reach out and grab objects instead of pointing and clicking\n\n---\n\nIf you make a prefab\n- and make a child prefab\nthat child will be loaded when the scene loads, and so it is ready to go instantly whenever you need it\n- so if this is a Sound, that sound will be ready to play\n- becuase Play On Awake is faster than calling Play Sound\n- s by instantiaing the Bell Sound prefab, and it has the sound which will \"Play On Awake\" then it'll play instantly \n\n![](/assets/images/2022-03-17-14-01-05.png)\n","n":0.041}}},{"i":22,"$":{"0":{"v":"To Do","n":0.707},"1":{"v":"\n[ ] Note the portion about \n- how can we see this white border showing the UI area?\n- ![](/assets/images/2022-03-08-14-28-02.png)\n\n\nTo ask\n- GameObject playing Video\n\n\nIdeas:\nImage target\n- identify image of boat in water\n- animate the photo of just the boat moving\n- add a gameobject underneath the boat with just a photo of water\n\nAsk\n- how to query camera frames in c# against the YOLO model?\n\n\n","n":0.128}}},{"i":23,"$":{"0":{"v":"Texture","n":1},"1":{"v":"\n> A texture is an image that is applied over the mesh surface. \nhttps://docs.unity3d.com/Manual/Textures.html\n\nTextures are applied using [[Unity.Material]]\n\n","n":0.236}}},{"i":24,"$":{"0":{"v":"Temp Note","n":0.707}}},{"i":25,"$":{"0":{"v":"Shader","n":1},"1":{"v":"\nA shader is part of a [[Unity.Material]]\n\n","n":0.378}}},{"i":26,"$":{"0":{"v":"Scene","n":1}}},{"i":27,"$":{"0":{"v":"Scene View","n":0.707},"1":{"v":"\n> You build the game in the scene view.\n\nThink of it as the editor in VS Code\nand the Game View as the markdown preview ","n":0.204}}},{"i":28,"$":{"0":{"v":"Renderer","n":1}}},{"i":29,"$":{"0":{"v":"Render Queue","n":0.707},"1":{"v":"\n\nProperties\nfrom: https://docs.unity3d.com/ScriptReference/Rendering.RenderQueue.html\n\nBackground\tThis render queue is rendered before any others.\n\nGeometry\tOpaque geometry uses this queue.\n\nAlphaTest\tAlpha tested geometry uses this queue.\n\nGeometryLast\tLast render queue that is considered \"opaque\".\n\nTransparent\tThis render queue is rendered after Geometry and AlphaTest, in \nback-to-front order.\n\nOverlay\tThis render queue is meant for overlay effects.\n\n\n\nYou can use these in creative ways.\n\nYou can place a transparent objedct inbetween then camera and some opaque object.\n\nThen set the render queue of the opaque object to be a higher value, such as 3001. \n\nThis enable Unity to crop the portion of the opaque object \n\n\n","n":0.107}}},{"i":30,"$":{"0":{"v":"Publish Game","n":0.707},"1":{"v":"\nHow to publish with WebGL.\nEasy - select Publish with WebGL\n\n<iframe id='webgl_iframe' frameborder=\"0\" allow=\"autoplay; fullscreen; vr\" allowfullscreen=\"\" allowvr=\"\"\n    mozallowfullscreen=\"true\" src=\"https://play.unity3dusercontent.com/webgl/fe3dbf40-3b78-49df-b1e3-afdbd682be02?screenshot=false&embedType=embed\"  width=\"810\"\n    height=\"640\" onmousewheel=\"\" webkitallowfullscreen=\"true\"></iframe>\n\n","n":0.209}}},{"i":31,"$":{"0":{"v":"Project View","n":0.707},"1":{"v":"\nThis shows you access to assets you have on your machine. \n- audio\n- 3D models\n- Scenes\n- Textures\n- etc\n","n":0.236}}},{"i":32,"$":{"0":{"v":"ProBuilder","n":1},"1":{"v":"\n\nDefault is cube\n\nNew shape\n- creates cube\n\nhttps://unity.com/features/probuilder\n\n","n":0.408}}},{"i":33,"$":{"0":{"v":"Pink Objects","n":0.707},"1":{"v":"\nIf the [[Unity.Game Object]] looks pink like this\n\n![](/assets/images/2022-02-24-19-55-56.png)\n\nThen it is from an outdated version.\nSelect:\n![](/assets/images/2022-02-24-19-56-18.png)","n":0.267}}},{"i":34,"$":{"0":{"v":"Particle Systems","n":0.707}}},{"i":35,"$":{"0":{"v":"Orientation","n":1}}},{"i":36,"$":{"0":{"v":"Order of Updates","n":0.577},"1":{"v":"\n![](/assets/images/2022-02-22-13-55-20.png)","n":1}}},{"i":37,"$":{"0":{"v":"Orbits","n":1},"1":{"v":"\n[[Normalize Data]]\n\nSo here, we want to make a Planet [[Unity.Game Object]] orbit the sun.\n\nThe RotateAround function takes\n- position of target to rotate around\n- vector direction\n- how far to move in angle degrees\n- - if we put in 360, we're going to completely circumnavigate the orbit. \n\n![](/assets/images/orbit-work-in-progress.mov)\n\n\n","n":0.147}}},{"i":38,"$":{"0":{"v":"OnCollisionEnter","n":1},"1":{"v":"\nFor the OnCollisionEnter function to work, both objects need a [[Unity.Physics.Rigidbody]]\n\n","n":0.302}}},{"i":39,"$":{"0":{"v":"Meshes, Materials, Shaders and Textures","n":0.447},"1":{"v":"\nFrom https://docs.unity3d.com/ru/2019.4/Manual/Shaders.html\n> Rendering in Unity uses Meshes, Materials, Shaders and Textures. They have a close relationship.\n- Meshes [[Unity.Mesh]] are the main graphics primitive of Unity. They define the shape of an object.\n- Materials [[Unity.Material]] define how a surface should be rendered, by including references to the Textures it uses, tiling information, Color tints and more. The available options for a Material depend on which Shader the Material is using.\n- Shaders [[Unity.Shader]] are small scripts that contain the mathematical calculations and algorithms for calculating the Color of each pixel rendered, based on the lighting input and the Material configuration.\n- Textures [[Unity.Texture]] are bitmap images. A Material can contain references to textures, so that the Material‚Äôs Shader can use the textures while calculating the surface color of a GameObject. In addition to basic Color (Albedo) of a GameObject‚Äôs surface, Textures can represent many other aspects of a Material‚Äôs surface such as its reflectivity or roughness.\n\n> - A Material specifies one specific Shader to use, and the Shader used determines which options are available in the Material. A Shader specifies one or more Texture variables that it expects to use, and the Material Inspector in Unity allows you to assign your own Texture Assets to these Texture variables.\n- For most normal rendering (such as rendering characters, scenery, environments, solid and transparent GameObjects, hard and soft surfaces) the Standard Shader is usually the best choice. This is a highly customisable shader which is capable of rendering many types of surface in a highly realistic way.\n- There are other situations where a different built-in Shader, or even a custom written shader might be appropriate (for example liquids, foliage, refractive glass, particle effects, cartoony, illustrative or other artistic effects, or other special effects like night vision, heat vision or x-ray vision).","n":0.058}}},{"i":40,"$":{"0":{"v":"Mesh","n":1},"1":{"v":"\nMeshes and [[Unity.Material]] work together. \n\nMaterial tells Unity what the thing looks like.\nMesh tells Unity what the shape of the thing is.\n\n\n","n":0.213}}},{"i":41,"$":{"0":{"v":"Material","n":1},"1":{"v":"\nNote to self... if you change the Base color of a material, and that material is applied to other Game Objects, then their color will also change \n\nA Material can be assigned to a [[Unity.Renderer]]\n\nA material is applied to a [[Unity.Mesh]]\n\n","n":0.156}}},{"i":42,"$":{"0":{"v":"Unity on Mac vs Windows","n":0.447},"1":{"v":"\n\n\"Preview Mode\"\n- A feature of Unity that is not available on Macs\n\nAs I understand it, on a Windows machine, you can connect an Oculus (excuse me... Meta) Quest or Quest 2 via their cable, click Play in Unity, and preview what you are building on the VR headset in real time.\n\nOn Mac, you have to first compile, then deploy to the device. \n\nIt sounds like installing Windows on a Mac doesn't remove this extra step. \n\nI'll update this note as I verify these. \nI do not currently, Feb 3, 2022, have a Windows computer. Just...3 Macs. ","n":0.102}}},{"i":43,"$":{"0":{"v":"Locomotion","n":1}}},{"i":44,"$":{"0":{"v":"Inspector","n":1},"1":{"v":"this is a context-sensitive window.\nClick on something and all the info shows up in the inspector.\n","n":0.25}}},{"i":45,"$":{"0":{"v":"Hungarian Notation","n":0.707},"1":{"v":"\nHungarian Notation adds a letter before the variable name to indicate the type of the variable.\n\nIn Unity, we can use \"m_\" before the variable name to indicate \"member.\"\n\n\n\n","n":0.189}}},{"i":46,"$":{"0":{"v":"Hierarchy Window","n":0.707},"1":{"v":"\nThis shows  a list of all the assets being used in the scene.s\n\nYou can group them to make [[Unity.Game Object]] Families.\n","n":0.218}}},{"i":47,"$":{"0":{"v":"Getting Started","n":0.707},"1":{"v":"\n> Unity interface has 5 windows\n1. [[Unity.Scene View]]\n2. Project Window\n3. [[Unity.Hierarchy Window]]\n4. [[Unity.Inspector]]\n5. [[Unity.Game View]]\n\n\n","n":0.258}}},{"i":48,"$":{"0":{"v":"Game View","n":0.707},"1":{"v":"\nThis is how your play, pause, or step through each frame.\n","n":0.302}}},{"i":49,"$":{"0":{"v":"Game Object","n":0.707},"1":{"v":"\n> Everything within a [[Unity.Scene]] is considered a Game Object.\n\nGame Object: \n- made up of a number of components\n- has a Transform\n- - position, rotation, scale of the game object\n- are seen in the [[Unity.Hierarchy Window]] view as a list\n- - dropdowns indicate parent-child relationships\n\nEverything you see in the [[Unity.Scene View]] is represented in the Game Object [[Unity.Hierarchy Window]]\n","n":0.13}}},{"i":50,"$":{"0":{"v":"Early Learnings","n":0.707},"1":{"v":"\nTrying to build a few \"robots\" in Unit.\n\n![](/assets/images/2022-02-22-23-14-27.png)\n","n":0.354}}},{"i":51,"$":{"0":{"v":"Collision","n":1},"1":{"v":"\nIf we want a [[Unity.Game Object]] to react to something bumping into it:\n- that object needs to have a [[Unity.Physics.Rigidbody]]\n- the object bumping, or being bumped, needs to have an 'OnCollisionEnter' function\n\n\n```\n// called in 1 frame when collision first is detected\n    // \n    private void OnCollisionEnter(Collision collision)\n    {\n        \n    }\n```\n","n":0.141}}},{"i":52,"$":{"0":{"v":"Collider","n":1}}},{"i":53,"$":{"0":{"v":"Camera Offset","n":0.707}}},{"i":54,"$":{"0":{"v":"C# Scripts","n":0.707},"1":{"v":"\n[HideInspector]\n>Makes a variable not show up in the inspector but be serialized.\nhttps://docs.unity3d.com/ScriptReference/HideInInspector.html\n\n[SerializeField]\n> Force Unity to serialize a private field.\nhttps://docs.unity3d.com/ScriptReference/SerializeField.html\n\nThe Start function only runs in the first frame\n\n```\n// Start is called before the first frame update\nvoid Start()\n{       \n}\n```\n\n\n\nThe Update function runs in every frame\n\n```\n// Update is called once per frame\nvoid Update()\n{\n}\n```","n":0.141}}},{"i":55,"$":{"0":{"v":"Asset Store","n":0.707},"1":{"v":"\nWhile Unity has moved to using the Package Manager, it is easier to search online through the [[Unity.asset]] store.\n\nYou can search online for assets, including free items here:\nhttps://assetstore.unity.com\n\nThen you can\n1. select Add to My Assets\n2. Then use Unity's \"package manager\" to import.\n\nYou can either:\n- Open In Unity\n- Download locally then Import\n\nYou can \n","n":0.137}}},{"i":56,"$":{"0":{"v":"AR Foundation","n":0.707},"1":{"v":"\nThis is an abstraction of ARCore and ARKit\n\nCode once, deploy to iOS and Android.\n\nCreating point cloud.\n\nEdit\nProject Settings\nXR Plug-In Management\n- ARKit\nEnable\n\n\nIf you're using URP\nSettings\nForward Renderer\nAdd\nAR Background Renderer Feature\n![](/assets/images/ar_foundation_forward_renderer.png)\n\n\nDelete the Main Camera\n\nCreate GameObjects\n- AR Session Origin\n- AR Session\n\n\nDownloading an animated character from Mixamo\n![](/assets/images/mixamo.png)\n\nDrag & Drop downloaded FBX into Unity\n\nExtract Textures\n\n\nShader ![](/assets/other/ShadowTransparent.shader.shader","n":0.143}}},{"i":57,"$":{"0":{"v":"AR Button as Child","n":0.5},"1":{"v":"\nAdding an AR button as a child object.\n\n\n![](/assets/images/AR_button_1.png)\n\n![](/assets/images/AR_button_2.png)\n\n![](/assets/images/AR_button_3.png)\n\n![](/assets/images/AR_button_4.png)\n\n![](/assets/images/AR_button_5.png)","n":0.354}}},{"i":58,"$":{"0":{"v":"1 on 1","n":0.577},"1":{"v":"\nNotes from call\n\nVR full screen\n\n3D content in AR\n- how to get location in x,y,x?\n\nWindow -> \n\nXR Plugin Management\n\n\nhttps://www.youtube.com/watch?v=VRiZmeExFiI&t=5s\n\n\n--\n\nIf you make an object\n- and change it up\n\n\nthen make it a child \n- it will inherit the parameters of the \n\n---\n\nMar 9, 2022 at 12:49:24 PM\n\nvideo \n_Basemap\n\nVideo Player is influencing the Texture \nBut URP \nMake a new Material which is Unlit \nUnlit means don't take into account lighting in the environment\n\nMaterials - if you drag a new material onto a Game Object, it will replace the pre-existing material.\n\nImage Targets\nWhen it recognizes the Image\n- it Mesh Renderer\n- it enable colliders\n- it enable UI elements\n\n2 Unity Events coded into Vuforia \n- on target found\n- on target lost \n\nSeparate Material for each video plane \n- Texture material for each video which is the first frame. \n- \n\nEach Material \n\n\n---\n\nIntro to procedural generation particle systems\n\n\nfamily tree\nslightly animated\ncan that be pre-built into a 3d model?\n\n\nis there hand tracking in AR with iOS?\nbarracuda ML learning for hand tracking \nwith AR foundation \nhe'd still use AR foundation\n\n\nbehind in the challenges\ncan we work through one together that you think is most important to get up to speed?\n\n\nar material \nempty in front\n\nzach lieberman? \nspanwing gameobjects based off user input\nas you're reading out what they are saying\nkeep appending the UI element\n\nonTextAdded\nonSpeechAdded\n\n\n2\naudio in \nhandheld library\n.microphone\nactivate and provide permissions\ntransform that \nhttps://docs.unity3d.com/ScriptReference/Microphone.html\n\nhow does vuforia calculate an image target \n\n--\n\nQ's \n\ndoes Unity, using rotate, recalculate the rotation after 1 rotation is completed\n\n\n","n":0.065}}},{"i":59,"$":{"0":{"v":"TypeScript","n":1},"1":{"v":"\n### *These are my notes as I learn TypeScript, and thus brush up on modern JavaScript.* \n<br>\n\nTypeScript compiles down to JavaScript.\n\nDetects errors in code without running it.\nIt does this by checking types. \n\nTypeScript is a superset of javascript. \n\nEverything in javascript is valid in TypeScript\n\nDeclare a type of a variable. \n(This looks just like Swift.)\n>let myVar: type = value\n\n`let songName: string = \"Green Grass\";`\n\n### Boolean\n`let isDone: boolean = false;`\n\nCompile by running: </br>\n`tsc file_name.ts`\n\n\n### Type Inference\nTypeScript has type inference.\nIf you explicitly create a variable and set it equal to \"hello,\" then TypeScript will infer it a string.\nIt is helpful to specify the type, because:\n>When you don‚Äôt specify a type, and TypeScript can‚Äôt infer it from context, the compiler will typically default to any.\n\n\n### \"Any\" type\nThis defeats the purpose of TypeScript.\n>TypeScript also has a special type, any, that you can use whenever you don‚Äôt want a particular value to cause typechecking errors. </br>\nWhen a value is of type any, you can access any properties of it (which will in turn be of type any), call it like a function, assign it to (or from) a value of any type, or pretty much anything else that‚Äôs syntactically legal:\n\n##xw# Function Definitions\n\n```\nfunction square(num: number) {\n    return num * num;\n}\n\nfunction greet(person: string){\n    return `Hi there, ${person}`\n}\n```\n\n*To provide default value:* <br>\n\n```\nfunction greet(person: string = \"stranger\"){\n    return `Hi there, ${person}`\n}\n```\n\n### Inferring vs Specifying Return Types\n\nThe return value annotation below, \": string\",  specifies the return type as string\n```\nfunction greet(person: string = \"stranger\"): string {\n    return `Hi there, ${person}`\n}\n```\n\n### Specifying void return types\n```\nfunction printIt(strToPrint: string): void{\n    console.log(strToPrint)\n}\n```\n\n### Never\n\"Never\" is unique to TypeScript.\nIt is not void.\nIndicates the function should never return anything. \nThis might be a function that logs an error, or it might be a loop that should never stop.\n\n\n### Objects\n> In JavaScript, the fundamental way that we group and pass around data is through objects. In TypeScript, we represent those through object types.\n\n```\ninterface Person {\n  name: string;\n  age: number;\n}\n\n```\n```\ntype Person = {\n  name: string;\n  age: number;\n};\n```\n```\nfunction greet(person: Person) {\n  return \"Hello \" + person.name;\n}\n\n```\n\n> Much of the time, we‚Äôll find ourselves dealing with objects that might have a property set. In those cases, we can mark those properties as optional by adding a question mark (?) to the end of their names.\n\n```\ninterface PaintOptions {\n  shape: Shape;\n  xPos?: number;\n  yPos?: number;\n}\n\n``` \n\n### Type Alias\n\n\n```\ntype Point = {\n    x: number;\n    y: number;\n}\n\nlet coordinate: Point = {x: 32, y: 6};\n```\n\n\n## Intersection Types\nThis is interesting - I hadn't seen this idea before.\nIt's kind of like 'extends' in Java?\n\n>interfaces allowed us to build up new types from other types by extending them. TypeScript provides another construct called intersection types that is mainly used to combine existing object types.\n\nAn intersection type is defined using the & operator.\n\n```\ninterface Colorful {\n  color: string;\n}\ninterface Circle {\n  radius: number;\n}\n \ntype ColorfulCircle = Colorful & Circle;\n``` \n> Here, we‚Äôve intersected Colorful and Circle to produce a new type that has all the members of Colorful and Circle.\n\n### Data Structures\n\n#### Array <br>\n>TypeScript, like JavaScript, allows you to work with arrays of values. Array types can be written in one of two ways. In the first, you use the type of the elements followed by [] to denote an array of that element type:\n\n`let numsList: number[] = [1, 2, 3];`\n\nor\n\n`let list: Array<number> = [1, 2, 3];`\n\n\n### Union Types\n> TypeScript‚Äôs type system allows you to build new types out of existing ones using a large variety of operators. Now that we know how to write a few types, it‚Äôs time to start combining them in interesting ways.\n\n> The first way to combine types you might see is a union type. A union type is a type formed from two or more other types, representing values that may be any one of those types. We refer to each of these types as the union‚Äôs members.\n\nAt first glance...I am not sure in what context this feature would be useful. Maybe I'll come across it somewhere.\n\n```\nfunction printId(id: number | string) {\n  if (typeof id === \"string\") {\n    // In this branch, id is of type 'string'\n    console.log(id.toUpperCase());\n  } else {\n    // Here, id is of type 'number'\n    console.log(id);\n  }\n}\n```\n\n\n### Literal Type\nconst zero: number = 0;\n\n\n### Some Examples\nvariable that can be a number or boolean\n`let highScore: number | boolean;`\n\n\narray that can hold numbers or strings, but not a mixture <br>\n`let stuff: number[] | string[] = [];`\n\n\ntype that holds 4 possible values\n```type SkillLevel = \n    \"Beginner\" |\n    \"Intermediate\" |\n    \"Advanced\" |\n    \"Expert\";\n```\n\n```\ntype RGB = {\n    r: number;\n    g: number;\n    b: number;\n}\n\ntype HSL = {\n    h: number;\n    s: number;\n    l: number;\n}\n\n// array called colors that can hold a mixture of RGB and HSL\nconst colors: (RGB | HSL)[] = [];\n```\n\n\n### Tuples\nThis is a TypeScript implementation. In JavaScript it transpiles to an array.\ns\n> Tuple can contain two values of different data types.\n\n```\nvar empId: number = 1;\nvar empName: string = \"Steve\";        \n\n// Tuple type variable \nvar employee: [number, string] = [1, \"Steve\"];\n\n```\n\n> A tuple type variable can include multiple data types as shown below.\n\n```\nvar user: [number, string, boolean, number, string];// declare tuple variable\n```\n\n\n> You can declare an array of tuple also.\n```\nvar employee: [number, string][];\nemployee = [[1, \"Steve\"], [2, \"Bill\"], [3, \"Jeff\"]];\n```\n\n### Enums\n\n```\nenum Direction {\n  Up = 1,\n  Down,\n  Left,\n  Right,\n}\n```\n\n\n### Interfaces\nThey allow us to define the structure of objects. \n\n```\ninterface Point {\n  readonly id: number; // readonly makes it so it cannot be changed\n  x: number;\n  y: number;\n  z?: number; // ? makes this optional\n  sayHello: () => string // this indicates the implementation of this interface must include the 'sayHello' method that returns a string and takes no parameters\n\n  sayBye(): string; // another way to write - this indicates the implementation of this interface must include the 'sayBye' method that returns a string and takes no parameters\n\n  changeNums(change: number): number; // this indicates the implementation of this interface must include the 'changeNums' method that returns a string and takes a parameter which is a number. The passed in param name does not need to be called 'change.'\n\n}\n```\n\n---\n\n### Aspects Related to Building Projects with TypeScript\n\n#### as always, a TODO List\n\n`mkdir project_name\n\ncd project_name\n\ntsc --init\n\ntsc -w `\n\nNow we want to be able to view this in the browser.\n\nTo make a live server:\n- keep in mind - below is not a function of TypeScript - this is just a normal npm server...\n- cd to the base directory of the project\n- create an npm package file: `npm init -y`\n- install lite server `npm install lite-server`\n- setup a script in the newly created npm 'package.json' so that npm start runs lite server, like this:\n- ![](/assets/images/2022-09-09-11-55-40.png)\n- run `npm start`\n- that should launch a browser\n- when you make a change in your .ts files of the project, they will compile to .js files, and the server is notified, and the changes appear live\n\n\nThis returns a generic HTML element. TS doesn't know this a Button\n\nAs often when working with the DOM, it's difficult to know if \"btn1\" will actually exist\n`const btn = document.getElementById(\"btn1\")`\n\nOne approach is to make them optional:\n```\nbtn?.addEventListener(\"click\", function ()) {\n    console.log(\"clicked\");\n}\n```\n\n---\n\nCalling tsc 'watch' in the terminal will cause node to automatically transpile the .ts into .js on save. <br>\n\n``` tsc -w ```\n> tsc-watch starts a TypeScript compiler with --watch parameter, with the ability to react to compilation status. tsc-watch was created to allow an easy dev process with TypeScript. Commonly used to restart a node server, similar to nodemon but for TypeScript.\n\n---\n\nPutting a hash symbol # in front of a property, or method, makes it private to a class. \n****\nThere is some debate here, because the 'private' typescript removes 'private' when it is converted to javascript.\n\nJavascript's way to do this is to use the hashtag in front of a parameter to make it private.\n\n`#price: number = 0`\n\n`private price: number = 0;`\n\n\n---\n\n## Loops\n\nThis looks the same as JavaScript to me\n```\nnums.forEach((element, index) => {\n        let tmpSum = 0;\n    })\n```\n\nHowever, since forEach is a function and not a loop, you can't use 'continue' or 'break' to break out of the loop.\n\nIf you may need to break the iteration, it's recommended you use something instead of forEach.\n\nUnfortunately, it's also show that hand-rolling for loops often runs quicker than built-in array functions such as map, forEach, etc.\n\n\n---\n\n## Array Operations\n\n### Slicing\nhttps://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Array/slice\n\nSlice creates a shallow copy of the array.\n\n**start** <br>\nZero-based index at which to start extraction.\n\n\n**end** <br>\nThe index of the first element to **exclude** from the returned array. Slice extracts up to but not including end. For example, slice(1,4) extracts the second element through the fourth element (elements indexed 1, 2, and 3). Thanks, Mozilla.\n\n","n":0.027}}},{"i":60,"$":{"0":{"v":"Python References","n":0.707},"1":{"v":"\n---\n\nDictionary operations\n\nNew empty dictionary\n```\nnew_dict = {}\n```\n\nRemove key and key's values from a dictionary. Just delete the key \n```\ndel my_dictionary['bill']\n```\n\n---\n\nfloor division vs modulo operator\n```\n#floor\n101 // 4 = 25\n\n#modulo\n101 % 4 = 1\n```\n---\n\nappend to a list vs extend a list\n(extend is what I expected append to do)\n```\nx = [1,2,3]\ny = [4,5]\n\nx.append(y)\n[1,2,3, [4,5] ]\n\nx.extend(y)\n[1,2,3,4,5]\n```\n\n---\n\n## List operations\n\nRemove item from list\n```\n# create a list\npeople = ['bill', 'john', 'sara']\n\n# remove from the list\npeople.remove('sara)\n```\n\n---\n### Python in VSCode\n\nTo select which version of Python to use, which means selecting a python *interpreter*\n- open the Command Palette\n![](/assets/images/2022-03-08-10-35-30.png)\n- then type: Python:Select Interpreter\n- click 'Python:Select Interpreter'\n- a drop down menu of python versions should appear\n- pick the version you want to use\n\n\n\n\n---\n\n## Virtual Environments\n\nWhy do we need virtual environments?\n- I'll refer to these below as 'venv'\n\nYou can have different versions of Python on your computer.\n\nYou can also have different versions of libraries. You might have a couple different of PyTorch. One project your working on might use a library which requires a specific version of another library. \n\nIf you just update some package, one fo your projects might stop working because it can't find a version of something it's looking for. Messy.\n\nVirtual environments let us create a folder in which we install the specific package versions we need for a certain project we're working on.\n\nThen if another project needs an updated version, we can create a virtual environment just for that project, install the required versions, and thus keep our projects required packages separate from each other.\n\nWith a virtual environment, we can also have separate versions of Python. Maybe one project needs Python 3.8, and another needs 3.10.\n\nHow to make one using command line:\n- create an empty folder\n- cd into that folder\n- run: `python -m venv env` or, if you want to specify the Python version such as  3.9, run `python3.9 -m venv env`\n\nIf you look in the folder you created to see what is there, within the 'bin' folder you will see the version of python which was used to create this virtual environment.\n\nNow we want to use this venv.\nThe word they chose for that action is to 'activate' the venv.\n\ncd into the env/bin/\n\nrun: `source activate`\n\nNow you should see: '(env) some_more_words_ bin'\n\nTo exit it, run: `deactivate`\n\nLet's start it back up again: run: `source activate`\nOk now that we're in here, we can `pip install` the packages we need for this project.\n\n","n":0.051}}},{"i":61,"$":{"0":{"v":"Normalize Data","n":0.707},"1":{"v":"\nWe often want to *normalize* our data so that the numbers are all between 0 and 1.\nWhy?\nBecause if you want to make a graph, or other cool picture, sometimes the tool you want to use doesn't accept very large or very small numbers.\n\nSo we need to shrink or grow our numbers to be within a range that the tool accepts.\n\nThis often, but not always, means changing the numbers relative to each other to be between zero and one.\n\nRight not, I am making a solar system in Unity for VR.\nI want the planets to orbit the sun with speeds relative to actual outer space speeds.\n\nLooking on wikipedia, I find the orbital speed of the planets around the sun:\n\n\nThen the formula for normalization says:\nWhats the biggest number in your data?\nWhat's the smallest number in your data?\n\nOk now, let's normalize each number in your data one at a time.\n\nlet max = the largest number in our data\nlet min = the smallest number in our data\nlet n = the number we want to normalize.\n\n(n - min)  / (max - min) = our 'n' normalized\n\nSo repeat the above formula for every number in your data to get between 0 and 1. \n\nIf you want to normalize this data between some other range. \nFor instance, if you want to make some virtual reality planet [[Unity.Orbits]] around a sun, then you would want to scale these values between 0 and 360, for the degrees in a circle.\n\nThe additional step is\n\nlet desired_min = the smallest of the number range you want the data to have\nlet desired_max = the largest of the number range you want the data to have\n\n (n - min)  / (max - min) * (desired_max - desired_min) + desired_min = our 'n' normalized to our desired range\n","n":0.059}}},{"i":62,"$":{"0":{"v":"Money & Finances","n":0.577},"1":{"v":"\n## TL;DR\n\nThe **number 1 tool** I found to stop get on the right track with money has been [YNAB](https://ynab.com/referral/?ref=gFqsRx-YRPm-j76D).\n\nThe second most useful tool in straightening out my approach to money and finances is Pete's blog called [mr money mustache.](https://www.mrmoneymustache.com)\n\n","n":0.16}}},{"i":63,"$":{"0":{"v":"Microphone Reviews","n":0.707},"1":{"v":"\nI _love_ microphones. I am always reading about, and trying mics. I know more about microphones than musical instruments. \n\nI'm going to see how well Dendron works for creating a page to compare mics. \nOther great resources:\n- https://marco.org/podcasting-microphones\n- https://podcastage.com\n\n---\n\n## RODE Videomic NTG to the RODE Wireless Go II\nComparing the RODE Videomic NTG to the RODE Wireless Go II's built in microphone.\nBoth were directly next to each other. \nConnected to RODE Connect mac app over usb-c.\nThe RODE Videomic NTG had 20db pad engaged, and 75hz low cut (which is how I would record with it.)\n\nI did no post processing except to match the levels.\n\n\n### RODE Videomic NTG </br>\n<audio src=\"/assets/recordings/voice_test_RODE_Videomic_NTG.m4a\" controls ></audio>\n\n\n### RODE Wireless Go II </br>\n<audio src=\"/assets/recordings/voice_test_Rode_Wireless_Go_II.m4a\" controls ></audio>\n\n---\n\n","n":0.092}}},{"i":64,"$":{"0":{"v":"Machine Learning","n":0.707},"1":{"v":"\n### Cosine Similarity\n\n> Cosine similarity measures the similarity between two vectors of an inner product space. It is measured by the cosine of the angle between two vectors and determines whether two vectors are pointing in roughly the same direction. It is often used to measure document similarity in text analysis.\nfrom: https://www.sciencedirect.com/topics/computer-science/cosine-similarity\n\nThis similarity measure isn't limited to computer vision, but we can use this in comparing two images, which are represented as vectors.\n\nHow similar are the two images?\nAre they \"going in the same direction?\"\n\n\n\n> ...it measures the cosine of the angle between two vectors projected in a multi-dimensional space...The smaller the angle, higher the cosine similarity.\nfrom: https://www.machinelearningplus.com/nlp/cosine-similarity/\n\n","n":0.097}}},{"i":65,"$":{"0":{"v":"Random Forest","n":0.707}}},{"i":66,"$":{"0":{"v":"Linear Algebra","n":0.707},"1":{"v":"\nFor an AR iOS app I'm making, I want to rotate a 3D model using Quaternions. After getting the quaternion rotation instruction completely wrong, I realized... hmmm I am definitely lacking something in my understanding. Why would it fly up to the ceiling?!?!\n\nI'm familiar with matrix manipulations, but I don't have the visual intuition for some of the operations. \nHappily, YouTube exists and I found the wonderful 3Blue1Brown.\n\nI'll be noting what I learn brushing up on Linear Algebra & Quaternions. \n","n":0.111}}},{"i":67,"$":{"0":{"v":"Matrix Manipulation","n":0.707},"1":{"v":"\nPlace this idea in your mind: a matrix is a transformation of space. \n\nTransformation & Function : different terms for the same idea.\nLet's think of a linear transformation as a function in a program, which takes in a vector and returns a vector. \nIt helps to imagine an input vector *moving* to its output vector.\nThen we can imagine each vector in the entire space moving in sync.\n\n- side note...can I make an AR app to visualize these?\n\nLinear Transformation\n- all lines must remain lines\n- - lines can't end up curved\n- - it should keep grid lines parallel and evenly spaced\n- the Origin must remain fixed in place\n\nA linear transformation is *entirely* determined by where it moves the basis vectors in the space.\n\nWe write this in a 2 x 2 matrix.\nColumn 1 is where i hat lands.\nColumn 2 is where j hat lands. \n\nIn 2D Space:\nSo, if we can describe the change to i hat and j hat, we can apply i hat & j hat's change to any vector and we'll know where that vector will land. And we can know where *every* other point will end up when the change is applied.\n\nx(where i hat ended up), + y(where j hat ended up) ... gotta install latex here\n\nAny other vector can be described as a linear combination of those basis vectors. \n\n> Every time you see a matrix, you can interpret it as a certain transformation of space. -3Blue1Brown\n\nWhen we do want to do more than one operation to a matrix, this is a Composition\n\nThis can still be described using just 1 matrix. \nWe can combine the 2 matrices that represent our individual operations into 1 matrix. \n\nIf you change the order that you multiply the matrices, your points will end up in a different place. \nMatrix multiplication is not commutative, but it is associative. \n\nAnd for 3 dimensions, use k hat, for a 3x3 matrix.\n\nDeterminant\nHow much a transformation scales the are of a matrix, is called the Determinant.\nCuts the area in half? The determinant is .5\nTriples the area? The determinant is 3.\nSpace becomes a line or one point? The determinant is 0. \n\nNegative determinant?\nInvert the orientation of space.\nThe absolute value of that determinant still tells you how the area of space has been scaled.\n\nIn 3D, the determinant changes the volume, instead of just the area.\n\nIf you have a negative determinant in 3D, some orientation has flipped.\n\n\nApplying a transformation, A, then A inverse, gets you back to where you started. \nA inverse, * A = the matrix that corresponds to the transformation that does‚Ä¶nothing. This is also called the ‚Äúidentity transformation.‚Äù\n[1,0\n0,1]\nIf there is an inverse, you can use that to solve the system.\n\nBut, there will be no inverse if a transformation effectively removes a dimension. If a 2D becomes a line (‚Äúrank 1‚Äù), or 3D becomes a plane(‚Äúrank 2‚Äù).\nThe ‚Äòrank‚Äô means the number of dimensions in the output transformation.\nOr, the numbers of dimensions in the column space.\nColumn space helps us understand if a solution even exists.\n\n\nAll the vectors that land on the origin (become null) are called ‚Äúnull space‚Äù or ‚Äúkernel.‚Äù\nThis helps know what the set of all possible solutions could look like.\n\nBelow, a 3 x 2 matrix (3 rows and 2 columns), is an example of a transformation from a 2D space to a 3D space.\nThis says: \n- take i hat, and move it to 4,2,5.\n- take j hat, and move it to 1,3,6\n| 4   | 1   |\n|-----|-----|\n| 2   | 3   |\n| 5   | 6   |\n\nAlternately if we are transforming from 3D to 2D, the matrix would look like:\n| 2   | 7   | 8   |\n|-----|-----|-----|\n| 4   | -3  | 9   |\n- take i hat, and move it to 2,4.\n- take j hat, and move it to 7,-3\n- take k hat, and move it to 8,9\n\n\n### The Importance Of Dot Products\n\n[4] * [2]</br>\n[1]\t[-1]</br>\n=(4 * 2) + (1 * -1)\n\n‚ÄúThis computation has a really nice geometric interpretation.‚Äù\nSummary: Dot product is a useful geometric tool for understanding projections, and for testing if vectors tend to point in the same direction.\n\n\n\n","n":0.039}}},{"i":68,"$":{"0":{"v":"Giving Up A Smartphone","n":0.5},"1":{"v":"\nFirst off...to prove I'm not some digital minimalist... <br>\n\n1. \nI am a full time augmented and virtual reality researcher / software programmer. I am an iOS developer, and I program augmented reality apps with Apple's ARKit. \nMy favorite podcast is atp, where they mostly talk about new Apple products. \nSo, I am not against iPhones.\nI will continue to devleop iOS augmented reality apps. \n\n2. \nSecond, I am not a skilled photographer, and I love that I can point my iPhone at anything and it will do a ton of work for me to get a photo that looks great and isn't blurry.\n\n3. \nThird, my whole family uses iMessage, and we send each other a lot of photos of our lives. \n\n4. \nFourth, I intend to increase the amount of youtube videos I make. I enjoy making them, and my iPhone has been my main camera. I just point it at what I want to film and it looks great to me. I can even sort of blur the background which looks cool.\n\n## But...The Problem <br>\nI do not like how much time I spend staring at the news, or youtube, or what other people have created on my iPhone, instead of creating something myself or reading the stack of books I always have lying around.\nI do not like the impression I am making on my young daughter who must think I \"always\" or \"often\" am looking at my phone. Whether I'm reading politico for no good reason, or texting a friend, or checking my bank account..those activities all appear the same to a young girl on the other side of the room who doesn't have a phone.\n\nI am aware there are people who create a lot of valuable things in the world, and manage to avoid getting sucked into a smartphone. see Cal Newport. And there are others who create interesting art and skip having a cell phone altogether: see Jack White.\nThen there are other artists who have switched to a dumber phone over time: Aziz Ansari, Ed Sheeran. \n\n## The Plan\nSo, I have begun my dumb phone search. \n\n## Jose Briones\nThis led me to Jose Briones who makes videos on this topic, and created https://josebriones.org/dumbphone-finder\n\n## Light Phone\n$300! <br>\nAn interesting one is the Light Phone. It has plenty of downsides, but the main thing is that it looks like a tiny Kindle. It uses e-ink.\n\nWhat I am concerned about is: they actively take and consider suggestions, and it's a fascinating object. \nThat makes me wonder if I will spend time admiring the Light Phone, reading about features they are considering, etc. \nFrom their site:\n> While considering additional functionality for the Light Phone II, it's important to remember all of the features it will never have. The Light Phone II will never have social media, internet browsing, email, news, or ads. \n\nAgain... $300 is a lot! <br>\n![](/assets/images/2022-10-21-16-52-24.png)\n\n<br>\n\n## Sunbeam Wireless\n$200 is also a lot.<br>\nWe can contrast that with Sunbeam Wireless, a company started by mennonites. \nThey take an off-the-shelf boring flip phone and customize the OS to be minimal. No web browser, no ability to add apps.\n![](/assets/images/2022-10-21-16-51-15.png)\n\n## Other\nWhat about just getting whatever Nokia is available,[ like Aziz did](https://www.calnewport.com/blog/2022/05/09/aziz-ansaris-digital-minimalism/)?\n![](/assets/images/2022-10-21-16-58-37.png)\n\nWell - those Nokia phones still have web browsers and youtube. \n\n---\n### Update on Nov 6, 2022\n\nSort of unsurprising for me, I have been experimenting with having multiple iPhones instead of 0 iPhones.\nI am trying an iPhone 12 Mini, without a SIM card, as my Camera + Spotify + Overcast machine.\n- It has the Freedom App installed which blocks all websites 24 hours/day\n- It has messages disabled, no email connection, no calendar connection.\n- it *does* have Apple Notes, Voice Memos, and my favorite note taking app: Bear\n\nAll notifications are disabled. All automatic updates are disabled.\n\nSurprising side effect - it is much snappier and seems to run more quickly than my \"phone\" which is also an iPhone 12.\n\nWill it work? We will see. \nSo far, using this has removed any feeling of foreboding that I usually get which is:\n'man...if I pick up that phone it's going to demand something of me: there's gonna be some message or worse some unknown message text for some list I'm on, or some app update, or whatever. \n\nSomething I am learning from this: I am much happier with completely zero notifications. I thought I was had tamed them on my iPhone, but this iPhone 12 mini approach of just turning all of it off is much more relaxing. \nI suppose if it also had a sim card, and I turned on messages, I would need to allow phone calls to prompt a notification, but I could live with messages not having notifications.\n\n\n### Update on Nov 27, 2022\nJust got back from a week vacation wih my family to Florida, including Universal Studios. \nWhat I tried was: I left my iPhone that has a SIM card at the hotel.\nI brought with me the iPhone 12 Mini which was my Camera and Audio player.\n- has no sim card\n- has no email\n- has Freedom app on a 24 hour session to block websites\n- does not receive SMS nor iMessage\n- does not receive Facetime\n- has all notifications off. \n\nThis was really close to being great. It runs very quickly, and I have zero sense of foreboding when I take this out of my pocket to use it. There is nothing to get distracted by. It is just acting as a camera and music/podcast player. The only time I found myself distracted by it was playing with customizing the wallpaper, etc.\nThe main con: my wife & I could not call or text each other. <- She did not like that.\n\n## Update on Jan 1, 2023\nThe amount of apps on this iPhone 12 mini is increasing...hmm.\nI tried, during Christmas break, to swap my sim card into this 12 mini, turn off my regular 12, and make this my regular phone. \nIt's better with all notifications, auto-updates, lock screen info turned off. It's definitely more pleasant, but I find myself using Safari to browse cnn for no reason. \n\nThe next hurdle when switching to a dumb phone will be spam calls. \nI mostly get spam calls. I've signed up for anything related to a do not call list that I can find, but I still get about 5 spam calls per day.\n\nI reached out to Light Phone folks to ask if their phone can silence unknown callers.\n\n> Me: <br>\n\"Does the light phone have this? Maybe it would be called:\n- \"Allow incoming calls from contacts only‚Äù <br>\nOr \n- \"silence unknown callers‚Äù\n\n>Light Phone support: <br>\n\"Thank you for your interest in the Light Phone. Other than being able to block individual numbers, we don't have any unique spam call filters at this time. We do hope to have a 'contacts only' call filter by the end of next year.\"\n\n\nI just ordered a Sunbeam F1 Orchid to try. <br>\nUgly looks, but:\n- can silence unknown callers\n- has maps\n- has no browser\n- voice to text\n\n\n## Update on Jan 14, 2023\nSwitching to the Sunbeam F1 Orchid as my cell phone has been better than I imagined. \n\nI had an important work project to complete and demonstrate on 1/13/2023, and I was done well ahead of time. I was done early enough that I went over what I created several times to improve it before the demo.\n\nIn the recent past, completing this kind of project always took working late, and sometimes on weekends to get the coding done on time. \n\nWhat does this have to do with talking about the Sunbeam F1?\n\nWell, I am attributing my diligence and ability to complete all this work *only during working hours* to the mental freedom, and the lack of easy distraction, that I was afforded by not having an internet search enabled connected device in my pocket all the time. \n\n#### First Impressions after week 1\nI turned off my iPhone, switched the SIM card to the Sunbeam, and that started the experiment!\n\nThis past week at work I had just the Sunbeam with me for texting and phone calls:\n1. The voice to text functionality is the best I've used. They say they use Microsoft's service for this. The automatic punctuation is even good.\n2. The touchscreen. I honestly forgot it would have a touchscreen, so this was a nice surprise. \n\nBut to the point - when work was difficult, and I'd usually take a break watching youtube, or checking cnn for no reason, the only thing I could do on the Orchid for entertainment was...change the wallpaper...from among the 5 or 6 low res images that come with it. (I went with the waves)\n\nMore benefits!\n- There are no apps on this thing to update. \n- There are no apps bugging me with notifications. \n\nNow the maps - I have not given these a solid trial. I used it once or twice while driving to navigate, and it works, but I need to check this out more thoroughly before recommending it or not.\n\nNow for the texting. Type with T9 is pretty awful. That's not for me. I'm easily able to type on the tiny qwerty touchscreen and there is automatic word suggestions I can select from while typing. But again, the voice to text is excellent, so I have been using that most of the time. \n\nBut what about iMessage???\nWell, I still a have an M1 MacBook air, and I still own my iPhone. \nI removed my cell phone number from iMessage, so it is only associated with an email address. People can iMessage me, and it'll show up on my MacBook. \n\nAlso, I am an iOS augmented reality developer. I am not selling my iPhone. I removed the SIM as mentioned, turned off iMessage, and tuned off all notifications.\n\nAnother part of this experiment, which has not been easy to solve, relates to music & podcasts. I have been using a Garmin vivoactive 4 as my watch for a couple years, partially because it has Spotify offline. This works...poorly. It technically works, but the limitations are severe. \nFor example: there is no fast forward or rewind. \nIf you use it to play a podcast, it does not remember your position. So if you start a long podcast, leave the garmin spotify 'app' to do something else, thn try to play that again it has no idea where you were and starts again at the beginning...and you cannot fast forward.\nSo, it is pretty awful for podcasts. \n\n\nAnd for me, in 2023, I'm treating things like this as experiments. If it doesn't end up working for me, or improving my day to day, then I will try something else.\nSo far, this week, switching to this is a definite improvement. \n\n\n","n":0.024}}},{"i":69,"$":{"0":{"v":"Git Commands Reference","n":0.577},"1":{"v":"\nSo there's some code out there you want to have on your computer.\nYou need to make a copy of it.\nThat's called 'clone'\n` git clone url_to_the_code_you_want `\n\nAfter cloning, you can see what is in your current folder with \n` ls -ltr `\n\nThen you need to change into that folder you cloned in order to work with it\n\n`\ncd folder_on_your_computer_where_the_code_was_cloned\n`\n\n\nThe place online where the code was stored is called the *origin*\n\n---\n\nIf you are working on a project on your computer, and want to use git for version control, you need to initialize git in the folder you are working on.\n\nFirst, open the terminal in the folder you want to use git in. Then run git init.\n\n`\ncd INTO_THE_FOLDER_YOU'RE_WORKING_IN\ngit init\n`\n\n---\n\nOnce you have done some work, whether creating new files you want git to track, making edits to existing files, you should run \"git add\" to \"stage\" the changes for the next \"commit.\"\n\n`git add NAME_OF_FILE `\n\nor, to just add everything that has changed in the folder, use \"git add . \" <- (the period matters)\n\n` git add . `\n\n---\n\nNext, you need to commit. That's what you always hear growing up, right? Commit.\nWhen you create a \"commit\" in git, you should include a brief explanation of what you changed in the commit, in human readable language, so your teammates (or, more likely, you in 1 week) can read this and know what the commit contained.\n\n`git commit -m \"This commit is to fix the bug about not enough text.\"`\n\nA commit is not yet a \"Git Push.\" That comes later. Push is what actually sends the bits o'er the innerwebs to GitHub, or wherever you are directing them. \n\n\n---\n\nIf you need to change the git commit message, you can use \"ammend.\"\n\n`git commit --amend -m \"Here is my corrected commit message\" `\n\n\nIf you created a commit, but forgot to add an edited file to the commit, use: \n\n```\ngit add FILE_NAME.TXT\ngit commit --amend --NO-EDIT\n```\n\n---\n\nIf you want to see the status of what files are accounted for in the commit, and which are not, you can ask for the status...\n\n`\ngit status\n`\n\n---\n\n### Git Diff \n\nAhhh git diff. Git Duff. üçª\n\nWhat is the difference between these two files I am looking at?\nWhat code is in John's file that is not in Sara's file?\n\nWhat did I delete today that was there yesterday?\n\nGit diff can show you.\n\nBelow, in a file called \"intro.txt,\" I hit the enter key, the words \"adding another line\", and then hit the enter key again.\n\n` git diff `\n\n![](/assets/images/2022-09-30-14-36-53.png)\n\n---\n\n### Git Pull\n\n### Switch to another branch\nIn git version 3.? they introduced 'switch' which saves a few steps, and is intended to clarify the many uses of git checkout.\n\n`\ngit switch type_branch_name_to_switch_to\n`\n\nif you want to create a new branch and switch to it\n`\ngit switch -c type_branch_name_to_switch_to\n`\n\n\n---\n\nTo, in one step, create a new branch and check it out \n\ngit checkout -b branch_name\n\nTo create a new branch from a branch called \"dev\" and switch to it:\n` git switch -c branch_name dev ` ","n":0.045}}},{"i":70,"$":{"0":{"v":"Computer Vision References","n":0.577},"1":{"v":"\n### Open CV\n\n...this is clearly a placeholder...","n":0.378}}},{"i":71,"$":{"0":{"v":"Books","n":1}}},{"i":72,"$":{"0":{"v":"The Lost City of Z","n":0.447},"1":{"v":"I read this after hearing about it on Rogan's podcast.\n\nRead in the Summer of 2022.\n\nI didn't take any notes, as there was no interesting advice to be gleaned from the book.\n\nLooking at the cover now, the only advice would be don't go into the jungle.\n\nThousands of people died over centuries going on expeditions into the Amazon jungle to search for legendary cities, or to search for people who went missing after going into the amazon on expeditions to search for legendary cities.\nThere are many ways to die in the jungle and the book makes it seem probable that if you go into the jungle you will die.\n\nIt is very well-written and I enjoyed it. I would recommend this as a fascinating book about a completely other way of spending your life.\n\n\n[Amazon link](https://www.amazon.com/Lost-City-Deadly-Obsession-Amazon-ebook/dp/B001NLL414/ref=sr_1_2?crid=1HPW7MERE6MMD&keywords=the+lost+city+of+z&qid=1660405015&sprefix=the+lost+city+of+z+%2Caps%2C70&sr=8-2)\n\n![](/assets/images/2022-08-13-11-36-32.png)","n":0.087}}},{"i":73,"$":{"0":{"v":"The Body: A Guide for Occupants","n":0.408},"1":{"v":"\nWow!\nI annoyed my family for week with this book. \nI kept shouting out facts or audible reactions to this book.\n\nThis was the first book I read by Bill Bryson, and after it I picked up two more. This is the kind of write that can make any topic interesting to read, like Michael Lewis or Malcolm Gladwell.\n\nI got this from the library, then bought my own copy because I wanted to mark it up with notes.\n\nI have many notes, and I will need to put them in here later.\n\n[Book Link](https://www.penguinrandomhouse.com/books/239783/the-body-by-bill-bryson/)\n\n![](/assets/images/2022-08-13-12-05-32.png)","n":0.105}}},{"i":74,"$":{"0":{"v":"Breathe by Rickson Gracie","n":0.5},"1":{"v":"I only recently (Jul 4, 2022 at 6:54:59 PM) learned about Brazilian Jiu Jitsu, and Rickson Gracie. I first heard his interview on Joe Rogan's podcast, then picked up this book from the library to bring on vacation.\n\nStarted and finished in 1 day! An easy and enjoyable book. I must have read this in July, 2022.\n\nBefore the book, I was trying to google to learn about the varying Gracie, and other BJJ school lineages. Was this a family split? Were they individual styles with different emphasis? This book had a good section explaining that yes, it was an unintended family divergence that led to multiple Gracie schools. This did not make it sound as dramatic as an unreconcilable rift, but more of a branching when someone didn't want to abide by another family member's rules.\n\nMy takeaway quotes:\n\n> Everything had been theoretical up to now, but now I was faced with new realities. My first **vale todo** fight taught me that that sometimes you don‚Äôt break physically ,but emotionally. Although I was already physically and mentally confident, I wasn‚Äôt spiritually and emotionally confident. If you don‚Äôt have the spiritual connection, you can‚Äôt dance on the razor‚Äôs edge. I made a vow to myself that from that day forward I would always try to cross the river no matter the consequences. This made a huge difference, not just in my fighting career, but in the way I looked at life. Moving forward, if I committed to something, I was resigned to the outcome no matter what it might be.\n\n\n\n[Amazon book link](https://www.amazon.com/Breathe-Life-Flow-Rickson-Gracie/dp/0063018950)\n\n![](/assets/images/2022-08-13-11-35-24.png)","n":0.062}}},{"i":75,"$":{"0":{"v":"Born to Run","n":0.577},"1":{"v":"Now here was a fascinating book!\nRead in the Summer of 2022.\n\nThe author (from Philadelphia! ...so bonus points from me) starts from a place of physical pain, and ends up running distances longer than a marathon. \n\n\n> \"Nelson Mandela...even in prison, continued to run seven miles a day in place in his cell.\"\n\n\n> ‚ÄúDon't fight the trail,\" Caballo called back over his shoulder.\n‚ÄúTate what if gives you If you have a choice between one step or two \nbetween rocks, take three:\" Caballo has spent so many years navigating\nin the trails, he's even nicknamed the stones beneath his felt‚Ä¶.\n\"Lesson two,\" Caballo called. \"Think Easy, Light, Smooth, and\nFast. You start with easy, because if that's all you get, that's not so bad.\nThen work on light. Make it effortless, like you don't give a shit how\nhigh the hill is or how far you've got to go. When you've practiced\nthat so long that you forget you're practicing, you work on making it\nsmoooooth. You won't have to worry about the last one-you get\nthose three, and you'll be fast.\"\nI kept my eyes on Caballo's sandaled feet, trying to duplicate his\nodd, sort of tippy-toeing steps. I had my head down so long, I didn't\nnotice at first that we'd left the forest.\n\"Wow!' I exclaimed.\nThe sun was just rising over the Sierras. Pine smoke scented the\nair, rising from dented stovepipes in the lodge-pole shacks on the\nedge of town. In the distance, giant standing stones like Faster Island\nstatues reared from the mesa floor, with snow-dusted mountains in\nthe background. Even if I hadn't been sucking wind, I'd have been\nbreathless.\n\"I told ya,\" Micah gloated.\n\n\n\n\n> \"According to our findings, currently available sports shoes ... are too soft and thick, and should be redesigned if they are to protect humans performing sports.\"\nUntil reading this study, I'd been mystified by an experience I'd\nhad at the Running Injury Clinic. I'd run back and forth over a force plate while alternating between bare feet, a superthin shoe, and the well-cushioned Nike Pegasus. Whenever I changed shoes, the impact levels changed as well--but not the way I'd expected. My impact forces were lightest in bare feet, and heaviest in the Pegs. My running form also varied: when I changed footwear, I instinctively changed my footfall. \"You're much more of a heel striker in the\nPegasus,\" Dr. Irene Davis concluded.\n\n\n> Dr. Bramble, meanwhile, was working a little higher up the evolu-\ntionary ladder with bigcats. He discovered that when many\nquadrupeds run, their internal organs slosh back and forth like water\nin a bathtub. Every time a cheetah's front feet hit the ground, its guts slam forward into the lungs, forcing out air. When it reaches out for\nthe next stride, its innards slide rearward, sucking air back in. Adding\nthat extra punch to their lung power, though, comes at a cost: it lim-\nits cheetahs to just one breath per stride.\nActually, Dr. Bramble was surprised to find that all running mam-\nmals are restricted to the same cycle of take-a-step, take-a-breath. In\nthe entire world, he and David could only find one exception:</br>\nYou. </br>\n\"When quadrupeds run,\nthey get stuck in a one-breath-per-\nlocomotion cycle,\" Dr. Bramble said. \"But the human runners we\ntested never went one to one. They could pick from a number of dif-\nferent ratios, and generally preferred two to one.\" The reason we're\nfree to pant to our heart's content is the same reason you need a\nshower on a summer day: we're the only mammals that shed most of\nour heat by sweating. All the pelt-covered creatures in the world cool\noff primarily by breathing, which locks their entire hear-regulating\nsystem to their lungs. But humans, with our millions of sweat glands,\nare the best air-cooled engine that evolution has ever put on the\nmarket.\n\n\nEpilogue\nThere is an article in the New York Times about the death of a main character in this book.\nhttps://www.nytimes.com/2012/05/21/sports/caballo-blancos-last-run-the-micah-true-story.html\n\n[Amazon Link](https://www.amazon.com/Born-Run-Christopher-McDougall-ebook/dp/B0028MBKVG/ref=sr_1_1?crid=4T9SA7EVQU3E&keywords=born+to+run&qid=1660406083&sprefix=born+to+run+%2Caps%2C73&sr=8-1)\n\n![](/assets/images/2022-08-13-11-56-06.png)","n":0.04}}},{"i":76,"$":{"0":{"v":"Ancillary Justice","n":0.707},"1":{"v":"\nA pretty good sci-fi book by Ann Leckie.\nRead in the late Summer of 2022.\n\n2014 Hugo & Nebula award winner. So...it should be good.\n\nAn interesting idea that is explored through the book: AI's reside in ships, and take over multiple human bodies to act as their living embodiments. They exist simultaneously in all the bodies and are connected via...tech?\n\nI did not keep track of all the characters - there were many and their names were difficult to remember.\n\nAlthough it wasn't mentioned specifically, I think all the characters were women.\n\n","n":0.107}}},{"i":77,"$":{"0":{"v":"3D User Interfaces","n":0.577},"1":{"v":"\nThis [book](https://www.eecs.ucf.edu/~jjl/) was recommended to me by Blair MacIntyre. \n\nThese notes are from the 2nd edition, published in 2017.\n\nChapter 1.1\n> \"A VR application may allow a user to place an object anywhere in 3D space, with any orientation-a task for which a 2D mouse is inadequate.\"\n\nRight away this reminds me of the [Brave NUI World](https://www.amazon.com/Brave-NUI-World-Designing-Interfaces/dp/0123822319) book. \n\nChapter 1.2\n> \"...development of 3D UIs is one of the most exciting areas of research in human-computer interaction (HCI) today.\n\n...when is \"today\" in this book? 2017? \nMy today is Feb 1, 2022 at 10:31:43 AM\n\nChapter 1.3: Terminology\n> Degrees of freedom: The number of independent dimensions of the motion of a body. \n\nAt the end of this chapter the authors arrive at this definition:\n>3D interaction: ...if a user tours a model of a building on her desktop computer by choosing viewpoints from a traditional menu, no 3D interaction has taken place. On the other hand, 3D interaction does not necessarily mean that 3D input devices are used...\n\nGood distinction between the input device and the type of interaction. I could use a 3D-capable input device to interact with a 2D environment, or a 2D input device to interact with a 3D environment.\n\nChapter 2.1: History of 3D UIs\n> ...unforseen usability issues\n\nTodo: read Sutherland(1968)\n\nOne of the first popular magazine covers about Virtual Reality wasn't about a VR headset, but instead showed the \"DataGlove\" which was a hand worn glove meant to manipulate a virtual environment.\n\nMany difficult details to work out in these devices:\n- how to know when a user is pointing to something in a VR environment?\n- how to handle the user putting a device down to use another input device?\n...side note: my Oculus Quest handles both of those quite well\n\n>designers of 3D UIs...still are faced with technological limitations, such as input latency, limited 3D workspace, tracing dropouts and cumbersome devices that must be worn, held, or attached.\n\n> Thus, the new subarea of HCI is termed 3D interaction, 3D user interface design, or 3D HCI\n\n- IEEE Symposium on 3D User Interfaces\n- ACM Symposium on Spatial User Interaction\n\nChapter 2.2: Roadmap to 3D UIs\n> The defining feature of 3D UIs is that users are viewing and acting in a real and/or virtual 3D space.\n\nInteraction Techniques for Composite and Application-Specific Tasks\n> ...complex tasks in 3D UI are often **composed** of the universal tasks...\n(emphasis mine)\n\n> ...the task of cloning objects in space could ne see as a **composite** task involving selection, system control, and manipulation, but there are benefits to considering this task independently an designing specific interaction techniques for it(Chen and Bowman 2009).\n(emphasis mine)\n\n> ...it is not trivial to put these elements together in a usable and understandable way\n\n\n\n**Three Important Terms**\n- presence\n> the feeling of \"being there\" tha you get when imersed in a virtual 3D world (Slater et al. 1994)\n- cybersickness \n> feelings of physical discomfort brought on by the use of immersive systems (Kennedy et al. 2000)\n- interaction fidelity\n> the level of realism in the UI (McMahan et al. 2012)\n\n\n2.2.3: Areas Impacted by 3D UIs\nMedicine and Psychiatry\n> For example, someone with a fear of snakes might be able to pick up and handle a virtual snake with a combination of 3D input devices and a realistic toy snake\n\n... this reminds me of Dzongsar Khentsye Rinpoche's story of the snake, and how a Buddhist teacher might try to work with a student with such a fear. \ntodo: insert that clip here\n\nOn HCI\nI'm going to quote this short paragraph in full:\n> The study of HCI has revealed many areas not addressed by traditional HCI. For example, what metrics should be used to study the user experience of a system? In typical UIs, metrics such as speed, accuracy, satisfaction, and perceived ease of use may be sufficient; in 3D UIs, we also need to asses things like physical comfort and presence. The development of heuristics or guidelines for good UI design is another area that has been studied thoroughly in traditional HCI but that requires further thought and expansion for 3D UIs. \n\nChapter 3\n> \"human factors\" refers to the capabilities, characteristics, and limitations the human user...\n\n>...we focus on three interconnected human factors categories perceptual, cognitive, and physical ergonomics issues \n\n>While attention is still not fully understood, evidence indicates it has three components: orienting to sensory events, detecting signals doe focused processing, maintaining a vigilant or alert state.\n\n>For example, consider a busy train station - a friend may be waving and, once you focus you attention on her, you may notice she is also calling your name. \n\n>...decision-making processes are in the center of information processing and depend on capturing, organizing, and combining information from various sources\n\n>Skills can be a major focus of a 3DUI...skill transfer is an important factor in many VR training applications, with the expectation that skills learned in a VE can also be applied in the real world\n\n*index of performance*\n- IP = (ID / MT)\n- - _index of difficulty_ / _movement time_\n\n> *index of performance*is expressed in bits per second and has been used to define the performance of many input devices. \n- - the hand itself = 10.6 bits/s\n- - a mouse = 10.4 bits/s\n- - a joystick = 5.0 bits/s\n- - a touchpad = 1.6  bits/s\n- - - well...I am much faster with this mac's trackpad than any mouse \n\n\n_steering law_\n...predictive model that describes the time to steer through a tunnel\n\n[ ]install latex and put in the formula\n\n\n**occlusion**\n> also called contour interruption or interposition\nthe phenomenon in which an object closer to the user partially obstructs the view of an object farther away\n\n**linear perspective**\n> the phenomenon what makes parallel lines appear to converge as they move away from the viewer\n\n...incidentally, this kind of phenomenon is what flat earthers use to 'prove' that the earth is flat. They say this is why you can't see objects which are actually beyond the horizon. They say no, they are just too far away to see, but that if you had a powerful enough telescope and nothing was in the way, you could Africa from NYC. See Eddie Bravo ranting on Rogan's show. Sigh. üôÑ\n\n\n**aerial perspective**\n> also called atmospheric attenuation\nis a cue that gauges relative distance by measuring the scattering and absorption of light through the atmosphere. For example, a nearer object will have more color saturation and brightness, while a more distant object will be duller and dimmer. \n\n\n**motion parallax**\n>depth information is conveyed when objects are _moving_ relative to the viewer\n\n\naudio - binaural cues\n>The fundamental problem with binaural cues id that there are locations relative to the listener's head where ambiguous situations occur...one of the ways...can deal with ambiguous binaural cues is with dynamic movement of the...head or the sound source itself.\n\nReverberation, of course, is a useful way to locate a sound source.\n\n**vestibular**\n>The vestibular system is closely tied to the ear and auditory nerves...the vestibular sense can be best understood as the human balance system. \n\n>...the vestibular system affects the control of the eyes, neck, and trunk/limbs\n\n>It is believed _cybersickness_ is caused by a mismatch between visual and vestibular cues.\n\n## somatosensory\n3.3.3\n\nThis is also called the haptic system.\n>The somatosensory, or haptic, system is an important perceptual system dealing with what we feel when touching or grasping objects and how we perceive the location and movement of the parts of our bodies. \n\n[ ]pickup here","n":0.029}}},{"i":78,"$":{"0":{"v":"Augmented Reality Projects","n":0.577}}},{"i":79,"$":{"0":{"v":"Field Drops","n":0.707},"1":{"v":"\n[Code on GitHub](https://github.com/BillMoriarty/field-drops/edit/main/README.md)\n\n### iOS Augmented Reality app experimenting with RealityKit for sound player interaction\n</br>\n\nThis little ARKit augmented reality app was made to try:\n- using RealityKit to generate shapes instead of importing pre-made assets\n- create a sound/music player that I can interact with with trigger sounds\n- using animation subscriptions to programmatically, continually animate a RealityKit object. I didn't expect to need to do this, but I wanted the objects to continually spin during sound playback, and it seems that functionality is not built in. [The animation subscription is found here](\nhttps://github.com/BillMoriarty/field-drops/blob/main/field-drops/ArWork.swift)\n\n\n\n\nI wanted shadows on the desk from the 3D objects, but I think I only partially achieved this effect. There is a light grey circle placed on a desk under the 3D objects, and an occlusion mask applied to the 3D objects so they appear to cause something like a shadow.\n\nThis app is on the  App store as [\"Field Drops.\"](https://apps.apple.com/us/app/field-drops/id1630444584)\n\n[![Field Drops demo](https://img.youtube.com/vi/6DTu30Ld5dU/0.jpg)](https://www.youtube.com/shorts/6DTu30Ld5dU)\n","n":0.082}}},{"i":80,"$":{"0":{"v":"Augmented Labels","n":0.707},"1":{"v":"\n[Code on GitHub](https://github.com/BillMoriarty/Augmented-Labels)\n\nThis is iOS code using SwiftUI + RealityKit + Computer Vision.\n\nThis is a demo to see if we can use SwiftUI with RealityKit and the Vision framework to:\n* recognize objects in the camera's view\n* create 3D text of what was recognized\n* place that 3D text in the camera's view near what was recognized\n\nHere is a code walk-through on YouTube.\n\n[![Augmented Labels Walkthrough](https://img.youtube.com/vi/WWYVHlEVkog/0.jpg)](https://www.youtube.com/watch?v=WWYVHlEVkog)\n\n","n":0.126}}},{"i":81,"$":{"0":{"v":"Apple Augmented Reality Notes","n":0.5},"1":{"v":"\n\n\n> The only way to get the automatic grounding shadow in RealityKit is to use the ‚Äúplane‚Äù AnchoringComponent.Target.¬†If you use the ‚Äúanchor‚Äù AnchoringComponent.target, you will not receive the automatic grounding shadow on your content, even if it is an ARPlaneAnchor. <br><br>\nYou can add your own grounding shadow by using a DirectionalLight with a Shadow and placing a plane beneath your model with an OcclusionMaterial, but you may not be able to achieve the same exact look as the automatic grounding shadows with this approach.<br><br>\n-Greg Chiste<br>\nDTS Engineer<br>\nÔ£ø Worldwide Developer Relations\n\n---\n","n":0.106}}},{"i":82,"$":{"0":{"v":"Session","n":1},"1":{"v":"\n\n> RealityKit automatically creates a default session that the view manages\n\n","n":0.302}}},{"i":83,"$":{"0":{"v":"Anchor Entity","n":0.707},"1":{"v":"\nAnchor entities are within RealityKit.\n","n":0.447}}},{"i":84,"$":{"0":{"v":"ARView","n":1},"1":{"v":"\nThe ARView contains the Scene. That means you do not need to create a Scene explicitly \n\nFrom Apple's [documentation](https://docs-assets.developer.apple.com/published/c54620eff873984d5217d35fecb71e70/ARView-1@2x.png):\n![](/assets/images/2022-10-26-09-41-07.png)\n\n","n":0.229}}},{"i":85,"$":{"0":{"v":"AR Anchor","n":0.707},"1":{"v":"\nAn [ARAnchor](https://developer.apple.com/documentation/arkit/aranchor) is continually updated by the iOS device as you move around to correct for any drift or miscalculation\n\nSo if you use ar anchors, your virtual content will stay much better in sync with the real world\n\nIt's a reference to the virtual world map.\n\nIt's important to notice that ARKit ARAnchors are different then RealityKit's [[Apple Augmented Reality Notes.Anchor Entity]]\n\nA good explanation from [stack overflow](https://stackoverflow.com/questions/57593960/whats-the-difference-between-aranchor-and-anchorentity):\n> The anchor in each of those examples are two different object types, the first is an ARAnchor, the second is an AnchorEntity. \n<br><br>\nIf you create an AnchorEntity like this: AnchorEntity(plane: .horizontal), then it will attach to the first horizontal ARAnchor which is automatically created with ARKit's plane detection. However if you instead create an AnchorEntity in this way: AnchorEntity(world: [0, 0, -1]), then it will position it at that [0, 0, -1] in world space, not using any ARAnchors.\n<br><br>\nYou might want to manually add an ARAnchor to the session if you want something to be positioned at the same place across two devices using collaborative sessions, but at an arbitrary location, such as [0, 0, -1] on one of the device's world space.\n","n":0.073}}},{"i":86,"$":{"0":{"v":"Advice for Artists","n":0.577},"1":{"v":"\n### Here are some collected quotes, thoughts, and advice from artists whose work I respect.\n\n\n> Be a good person, do dope things, and remember to enjoy it all. None of us make it out alive.\n- DJ Muggs\n\n\n> I prefer to shoot the arrow, then paint the target around it. You make the niches in which you finally reside.\n- Brian Eno\n\n> Music is just something interesting to do with the air.\n- Tom Waits\n\n> Make something that speaks to yourselves. And, hopefully someone else will like it, but you can't second guess your own taste for what someone else is going to like... it won't be good. We're not smart enough to know what someone else is going to like. \"Well, I don't really like it but I think this group of people will like it....\" it's a bad way to play the game of music or art. You have to do what's personal to you, take it as far as you can go, really push the boundaries, and people will resonate with it if they're supposed to resonate with it. But, you can't get there the other way. The other way is a dead end path.\nWhat makes it great is the personal, with all of its imperfections and quirkiness. \nHowever you see the world, that's different from how everyone else sees the world - that's why you're an artist. That's your purpose in sharing your work with the world.   \n- Rick Rubin\n\n\n","n":0.064}}}]}
